import shap
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

# ğŸ“Œ 1. Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Ï„Î¿Ï… dataset
df = pd.read_csv("train_dataset.csv")

# ğŸ“Œ 2. Î”Î¹Î±Ï‡Ï‰ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½ ÎºÎ±Î¹ labels
X = df.drop(columns=['slice Type']).values
y = df['slice Type'].values
y = y - 1  # ğŸ”¹ Labels Î±Ï€ÏŒ 0, 1, 2

# ğŸ“Œ 3. ÎšÎ±Î½Î¿Î½Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ğŸ“Œ 4. Î”Î¹Î±Ï‡Ï‰ÏÎ¹ÏƒÎ¼ÏŒÏ‚ ÏƒÎµ train ÎºÎ±Î¹ test sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)

# ğŸ“Œ 5. Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· Ï„Î¿Ï… XGBoost
xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=42)
xgb_model.fit(X_train, y_train)

# ğŸ“Œ 6. Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ SHAP Values
explainer = shap.Explainer(xgb_model, X_train)
shap_values = explainer(X_test)

# ğŸ“Œ 7. ÎŸÏ€Ï„Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· ÏƒÎ·Î¼Î±Î½Ï„Î¹ÎºÏŒÏ„Î·Ï„Î±Ï‚ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½
shap.summary_plot(shap_values, X_test, feature_names=df.drop(columns=['slice Type']).columns)

# ğŸ“Œ 1. Î’ÏÎ¯ÏƒÎºÎ¿Ï…Î¼Îµ Ï„Î· ÏƒÎ·Î¼Î±ÏƒÎ¯Î± Ï„Ï‰Î½ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½
importance = np.abs(shap_values.values).mean(axis=0).flatten()
feature_names = df.drop(columns=['slice Type']).columns.tolist()  # Î Î±Î¯ÏÎ½Î¿Ï…Î¼Îµ Ï„Î± Î¿Î½ÏŒÎ¼Î±Ï„Î± Ï„Ï‰Î½ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½
importance = np.abs(shap_values.values).mean(axis=0)  # Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ SHAP values

# ğŸ”¹ Î•Î»Î­Î³Ï‡Î¿Ï…Î¼Îµ Î±Î½ Î¿Î¹ Î´Î¹Î±ÏƒÏ„Î¬ÏƒÎµÎ¹Ï‚ Ï„Î±Î¹ÏÎ¹Î¬Î¶Î¿Ï…Î½
if len(feature_names) == len(importance):
    feature_importance = pd.DataFrame({'Feature': feature_names, 'Importance': importance})
    feature_importance = feature_importance.sort_values(by='Importance', ascending=False)
    print("ğŸ”¹ Î£Î·Î¼Î±Î½Ï„Î¹ÎºÏŒÏ„Î·Ï„Î± Î§Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½ ÏƒÏÎ¼Ï†Ï‰Î½Î± Î¼Îµ SHAP:")
    print(feature_importance)
else:
    print(f"âŒ Error: Feature names ({len(feature_names)}) and SHAP importance values ({len(importance)}) do not match!")

feature_importance = feature_importance.sort_values(by='Importance', ascending=False)

print("ğŸ”¹ Î£Î·Î¼Î±Î½Ï„Î¹ÎºÏŒÏ„Î·Ï„Î± Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½ ÏƒÏÎ¼Ï†Ï‰Î½Î± Î¼Îµ SHAP:")
print(feature_importance)

# ğŸ“Œ 2. Î‘Ï†Î±Î¹ÏÎ¿ÏÎ¼Îµ ÏƒÏ„Î±Î´Î¹Î±ÎºÎ¬ Ï„Î± Î»Î¹Î³ÏŒÏ„ÎµÏÎ¿ ÏƒÎ·Î¼Î±Î½Ï„Î¹ÎºÎ¬ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬
accuracies = []
num_features_list = []

for num_features in range(16, 3, -2):  # ÎÎµÎºÎ¹Î½Î¬Î¼Îµ Î¼Îµ ÏŒÎ»Î± ÎºÎ±Î¹ Î¼ÎµÎ¹ÏÎ½Î¿Ï…Î¼Îµ Î±Î½Î¬ 2 Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬
    selected_features = feature_importance['Feature'].iloc[:num_features].values
    X_reduced = df[selected_features].values
    X_scaled = scaler.fit_transform(X_reduced)

    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)

    xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=42)
    xgb_model.fit(X_train, y_train)
    
    y_pred = xgb_model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    
    accuracies.append(acc)
    num_features_list.append(num_features)

    print(f"ğŸ“‰ Features used: {num_features} | Accuracy: {acc:.4f}")

# ğŸ“Œ 3. ÎŸÏ€Ï„Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· Ï„Î·Ï‚ Î¼ÎµÎ¯Ï‰ÏƒÎ·Ï‚ Ï„Î·Ï‚ Î±ÎºÏÎ¯Î²ÎµÎ¹Î±Ï‚
plt.figure(figsize=(10, 5))
plt.plot(num_features_list, accuracies, marker='o', linestyle='-')
plt.xlabel("Number of Features Used")
plt.ylabel("Accuracy")
plt.title("Impact of Feature Reduction on Accuracy")
plt.grid()
plt.show()

